\chapter{Graph Theory}
In this chapter we are going to discuss three graph theoretical problems.
\begin{enumerate}
\item We present an algorithm to solve the 
      \href{https://en.wikipedia.org/wiki/Disjoint-set_data_structure}{\emph{union-find problem}}.
      In this problem, we are given a set $M$ and a relation $R \subseteq M \times M$.  Our task is
      then to find the smallest equivalence relation $R^\approx$ such that $R \subseteq R^\approx$.  
\item The next problem we solve is the problem to compute the
      \href{https://en.wikipedia.org/wiki/Minimum_spanning_tree}{\emph{minimum spanning tree}}
      of a graph.  Given a weighted graph, this problem asks to find the smallest 
      \href{https://en.wikipedia.org/wiki/Tree_(data_structure)}{tree} that 
      spans the graph.
\item Finally, we discuss the problem of finding a shortest path in a 
      \href{https://en.wikipedia.org/wiki/Directed_graph}{\emph{weighted directed graph}}.
\end{enumerate}

\section{The Union-Find Problem}
Assume that we are given a set $M$ together with a relation $R \subseteq M \times M$.  The relation
$R$ is not yet an  equivalence relation on $M$, but this relation \emph{induces} an equivalence relation
$\approx_R$ on $M$.  This \emph{induced equivalence relation} is defined inductively.
\begin{enumerate}
\item For every pair $\pair(x,y) \in R$ we have that $\pair(x, y) \in\; \approx_R$.

      This is the base case of the inductive definition.  It ensures that the relation
      $\approx_R$ is an extension of the relation $R$.
\item For every $x \in M$ we have $\pair(x,x) \in\; \approx_R$.

      This clause ensures that the relation $\approx_R$ is reflexive on $M$.
\item If $\pair(x,y) \in \approx_R$, then $\pair(y,x) \in\; \approx_R$.

      This clause ensures that the relation $\approx_R$ is symmetric.
\item If $\pair(x,y) \in\; \approx_R$ and $\pair(y,z) \in\; \approx_R$, then $\pair(x,z) \in\; \approx_R$.

      This clause ensures that the relation $\approx_R$ is transitive.
\end{enumerate}
Given this inductive definition, it can be shown that:
\begin{enumerate}
\item $\approx_R$ is an equivalence relation on $M$.
\item If $Q$ is an equivalence relation on $M$ such that $R \subseteq Q$, then $\approx_R \subseteq Q$.
\end{enumerate}
Therefore, the relation $\approx_R$ is the smallest
equivalence relation on $M$ that extends $R$.  In our lesson on linear
algebra we had defined the transitive closure $R^+$ of a binary relation $R$ in a similar way.  In
that lecture, we had then shown that $R^+$ is indeed the smallest transitive relation that extends
$R$.  This proof can easily be adapted to prove the claim given above.

It turns out that a direct implementation of the inductive definition of $\approx_R$ given above is
not very efficient.  Instead, we remind ourselves that there is are one-to-one correspondence
between an equivalence relations $R \subseteq M \times M$ and a
\href{https://en.wikipedia.org/wiki/Partition_of_a_set}{\emph{partition}} of $M$.  A set 
$\mathcal{P} \subseteq 2^M$ is a \emph{partition} of $M$ iff the following holds:
\begin{enumerate}
\item $\{\} \not\in \mathcal{P}$,
\item $A \in \mathcal{P} \wedge B \in \mathcal{P} \rightarrow A = B \vee A \cap B = \{\}$,
\item $\ds\bigcup \mathcal{P} = M$.
\end{enumerate}
Therefore, a partition $\mathcal{P}$ of $M$ is a subset of the power set of $M$ such that
every element of $M$ is a member of exactly one set of $\mathcal{P}$ and, furthermore, $\mathcal{P}$ must not contain the
empty set.  We have already seen in the lecture on Linear Algebra that an equivalence relation 
$\approx \;\subseteq M \times M$ gives rise to \emph{equivalence classes}, where the equivalence class
generated by $x \in M$ is defined as
\\[0.2cm]
\hspace*{1.3cm}
$[x]_\approx := \{ y \mid \pair(x, y) \in\; \approx \}$.
\\[0.2cm]
It was then shown that the set 
\\[0.2cm]
\hspace*{1.3cm}
$\bigl\{ [x]_\approx \;\big|\; x \in M \bigr\}$
\\[0.2cm]
is a partition of $M$.  It was also shown that every partition $\mathcal{P}$ of a set $M$ gives rise
to an equivalence relation $\approx_\mathcal{P}$ that is defined as follows:
\\[0.2cm]
\hspace*{1.3cm}
$x \approx_\mathcal{P} y \;\Longleftrightarrow\; \exists A \in \mathcal{P}:(x \in A \wedge y \in A)$.
\\[0.2cm]
An example will clarify the idea.  Assume that
\\[0.2cm]
\hspace*{1.3cm}
$M := \{ 1,2,3,4,5,6,7,8,9 \}$.
\\[0.2cm]
Then the set 
\\[0.2cm]
\hspace*{1.3cm}
$\mathcal{P} := \bigl\{ \{ 1, 4, 7, 9\}, \{3, 5, 8\}, \{2, 6\} \bigr\}$
\\[0.2cm]
is a partition of $M$ since the three sets involved are disjoint and their union is the set $M$.
According to this partition, the elements $1$, $4$, $7$, and $9$ are all
equivalent to each other.  Similarly, the elements $3$, $5$, and $8$ are equivalent to each other,
and, finally, $2$ and $6$ are equivalent.

It turns out that, given a relation $R$, the most efficient way to compute the induced equivalence
relation $\approx_R$ is to compute the partition corresponding to this equivalence relation.  In
order to present the algorithm, we first sketch the underlying idea using a simple example.  Assume
the set $M$ is defined as
\\[0.2cm]
\hspace*{1.3cm}
$M := \{ 1,2,3,4,5,6,7,8,9 \}$
\\[0.2cm]
and that the relation $R$ is given as follows:
\\[0.2cm]
\hspace*{1.3cm}
$R := \bigl\{ \pair(1,4), \pair(7,9), \pair(3,5), \pair(2,6), \pair(5,8), \pair(1,9), \pair(4,7) \bigr\}$.
\\[0.2cm]
Our goal is to compute a partition $\mathcal{P}$ of $M$ such that the formula
\\[0.2cm]
\hspace*{1.3cm}
$\pair(x, y) \in R \rightarrow \exists A \in \mathcal{P}:\bigl(x \in A \wedge y \in A)$
\\[0.2cm]
holds.  In order to achieve this goal, we define a sequence of partitions $\mathcal{P}_1$,
$\mathcal{P}_2$, $\cdots$, $\mathcal{P}_n$ such that $\mathcal{P}_n$ achieves our goal.
\begin{enumerate}
\item We start be defining
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathcal{P}_1 := \bigl\{ \{1\}, \{2\}, \{3\}, \{4\}, \{5\}, \{6\}, \{7\}, \{8\}, \{9\} \bigr\}$.
      \\[0.2cm]
      This is clearly a partition of $M$, but it is the trivial one since it induces an equivalence
      relation $\approx$ where we have  $x \approx y$ only if $x = y$.  
\item Next, we have to ensure to incorporate our given relation $R$ into this partition.  Since $\pair(1,4) \in R$
      we replace the singleton sets $\{1\}$ and $\{4\}$ by their union.  This leads to the following
      definition of the partition $\mathcal{P}_2$:
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathcal{P}_2 := \bigl\{ \{1, 4\}, \{2\}, \{3\}, \{5\}, \{6\}, \{7\}, \{8\}, \{9\} \bigr\}$.
\item Since $\pair(7,9) \in R$, we replace the sets $\{7\}$ and $\{9\}$ by their union and define
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathcal{P}_3 := \bigl\{ \{1, 4\}, \{2\}, \{3\}, \{5\}, \{6\}, \{7, 9\}, \{8\} \bigr\}$.
\item Since $\pair(3,5) \in R$, we replace the sets $\{3\}$ and $\{5\}$ by their union and define
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathcal{P}_4 := \bigl\{ \{1, 4\}, \{2\}, \{3,5\}, \{6\}, \{7, 9\}, \{8\} \bigr\}$.
\item Since $\pair(2,6) \in R$, we replace the sets $\{2\}$ and $\{6\}$ by their union and define
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathcal{P}_5 := \bigl\{ \{1, 4\}, \{2,6\}, \{3,5\}, \{7, 9\}, \{8\} \bigr\}$.
\item Since $\pair(5,8) \in R$, we replace the sets $\{3,5\}$ and $\{8\}$ by their union and define
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathcal{P}_6 := \bigl\{ \{1, 4\}, \{2,6\}, \{3,5,8\}, \{7, 9\} \bigr\}$
\item Since $\pair(1,9) \in R$, we replace the sets $\{1,4\}$ and $\{7,9\}$ by their union and define
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathcal{P}_7 := \bigl\{ \{1, 4, 7, 9\}, \{2,6\}, \{3,5,8\} \bigr\}$
\item Next, we have $\pair(4,7) \in R$.  However, $4$ and $7$ are already in the same set.
      Therefore we do not have to change the partition $\mathcal{P}_7$ in this step.
      Furthermore, we have now processed all the pairs in the given relation $R$.
      Therefore, $\mathcal{P}_7$ is the partition that represents the equivalence relation $\approx$ induced
      by $R$.  According to this partition, we have found that
      \\[0.2cm]
      \hspace*{1.3cm}
      $1 \approx 4 \approx 7 \approx 9$, \quad $2 \approx 6$,  \quad and \quad $3 \approx 5 \approx 8$.
\end{enumerate}
 

\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.0cm,
                  xrightmargin  = 0.0cm,
                ]
    unionFind := procedure(m, r) {
        p := { { x } : x in m };  // start with the trivial partition
        // refine this partition to accomodate all pair [x,y] in r
        for ([x, y] in r) {
            sx := find(x, p);
            sy := find(y, p);
            if (sx != sy) {
                p  -= { sx, sy };  // remove old sets
                p  += { sx + sy }; // add their union
            }
        }
        return p;
    };
    find := procedure(x, p) {
        return arb({ s : s in p | x in s });  
    };
\end{Verbatim}
\vspace*{-0.3cm}
\caption{A naive implementation of the union-find algorithm.}
\label{fig:union-find-naive.stlx}
\end{figure}

What we have sketched in the previous example is known as the \emph{union-find algorithm}.
Figure \ref{fig:union-find-naive.stlx} shows a naive implementation of this algorithm.  The
procedure \texttt{unionFind} takes two arguments: \texttt{m} is a set and \texttt{r} is a relation
on \texttt{m}.  The purpose of \texttt{unionFind} is to compute the equivalence relation
induced by \texttt{r}.  This equivalence relation is represented as a partition of \texttt{m}.
\begin{enumerate}
\item In line 2 we initialize \texttt{p} as the trivial partition that contains only singleton
      sets.  Obviously, this is a partition of \texttt{m} but it does not take the
      relation \texttt{r} into account.
\item The \texttt{for}-loop in line 4 iterates over all pairs \texttt{[x,y]} from \texttt{r}.
      First, we compute the set \texttt{sx} that contains \texttt{x} and the set \texttt{sy} that
      contains \texttt{y}.  If these sets are not the same, then \texttt{x} and \texttt{y} are not
      yet equivalent with respect to the partition \texttt{p}.  Therefore, the equivalence classes
      \texttt{sx} and \texttt{sy} are joined and their union is added to the partition in line 9, while
      the equivalence classes \texttt{sx} and \texttt{sy} are removed in line 8.
\item The function \texttt{find} takes an element \texttt{x} of a set \texttt{m} and a partition
      \texttt{p} of \texttt{m}.  Since \texttt{p} is a partition of \texttt{m} there must be exactly
      one set \texttt{s} in \texttt{p} such that \texttt{x} is an element of \texttt{s}.  This set
      \texttt{s} is then returned.
\end{enumerate}

\subsection{A Tree-Based Implementation}
The implementation shown in Figure \ref{fig:union-find-naive.stlx} is not very efficient.  The
problem is the computation of 
\\[0.2cm]
\hspace*{1.3cm}
\texttt{sx + sy}.
\\[0.2cm]
If the sets \texttt{sx} and \texttt{sy} are represented as binary trees, then this computation takes
time proportional to $\mathtt{min}(\mathtt{\#sx},\mathtt{\#sy})$.  Here $\mathtt{\#sx}$ denotes the size of
\texttt{sx} and similarly $\mathtt{\#sy}$ denotes the size of \texttt{sy}.  A more efficient way to
represent these sets is via \emph{parent pointers}:  The idea is that every set is represented as a
tree.  However, this tree is not a binary tree but is rather represented by pointers that
point from the a node to its parent.  The node at the root of the tree points to itself.  Then, taking the
union of two sets \texttt{sx} and \texttt{sy} is simple:  If \texttt{rx} is the node at the root of
the tree representing \texttt{sx} and \texttt{ry} is the node at the root of the tree representing
\texttt{sy}, then we can just change the parent pointer of \texttt{ry} to point to \texttt{rx}.

\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                ]
    find := procedure(x, parent) {
        if (parent[x] == x) {
            return x;
        }
        return find(parent[x], parent);
    };
    unionFind := procedure(m, r) {
        parent := { [n, n] : n in m };  
        for ([x, y] in r) {
            parentX := find(x, parent);
            parentY := find(y, parent);
            if (parentX != parentY) {
                parent[parentY] := parentX;  // create union
            }
        }
        roots := { x : x in m | parent[x] == x };
        return { { y : y in m | parent[y] == r } : r in roots };
    };
\end{Verbatim}
\vspace*{-0.3cm}
\caption{A tree-based implementation of the union-find algorithm.}
\label{fig:union-find-tree.stlx}
\end{figure}

Figure \ref{fig:union-find-tree.stlx} on page \pageref{fig:union-find-tree.stlx} shows an
implementation of this idea.  In this implementation, the parent pointers are represented using the
binary relation \texttt{parent}.  
\begin{enumerate}
\item The function \texttt{find} takes a node \texttt{x} and the binary relation \texttt{parent} 
      representing the parent pointers.  The purpose of the call \texttt{find(x, parent)} is to
      return the root of the tree containing \texttt{x}.

      If \texttt{x} is its own parent, then \texttt{x} is already at the root of a tree and therefore 
      we can return \texttt{x} itself in line 3.

      Otherwise, we compute the parent of \texttt{x} and recursively compute the root of the tree
      containing this parent.  
\item The function \texttt{unionFind} takes a set \texttt{m} and a relation \texttt{r}.  It returns
      a partition of \texttt{m} that represents the equivalence relation generated by \texttt{r} on
      \texttt{m}.

      The binary relation\footnote{
        In a language like \texttt{C} we would instead use pointers.  Of course, this would be more efficient.
      } \texttt{partition} is initialized in line 8 so that every node
      points to itself.   This corresponds to the fact that the sets in the initial partition are all
      singleton sets.  

      Next, the function \texttt{unionFind} iterates over all pairs \texttt{[x, y]} from the binary
      relation \texttt{r}.  In line 10 and 11 we compute the roots of the trees containing \texttt{x} and
      \texttt{y}.  If these roots are identical, \texttt{x} and \texttt{y} are already equivalent.
      Otherwise, the parent pointer of the root of the tree containing \texttt{y} is changed so that it
      now points to the root of the tree containing \texttt{x}.  Therefore, instead of iterating over all
      elements of the set containing \texttt{y} we just change a single pointer.

      Line 16 computes the set of all nodes that are at the root of some tree.  Then, for every root
      \texttt{r} of a tree, line 17 computes the set of nodes corresponding to this tree.
\end{enumerate}

\subsection{Controlling the Growth of the Trees}
As it stands, the algorithm show in the previous section has a complexity that is $\Oh(n^2)$ in the
worst case where $n$ is the number of elements in the set \texttt{m}.  The worst case happens if there
is just one equivalence class and the tree representing this class degenerates into a list.
Fortunately, it is easy to fix this problem.  We just have to keep track of the height of the
different trees.  Then, if we want to join the trees rooted at \texttt{parentX} and
\texttt{parentY}, we have a choice: We can either set the parent of the node \texttt{parentX} to
be \texttt{parentY} or we can set the parent of the node \texttt{parentY} to be \texttt{parentX}.
If the tree rooted at \texttt{parentX} is smaller than the tree rooted at \texttt{parentY} we should
use the assignment
\\[0.2cm]
\hspace*{1.3cm}
\texttt{parent[parentX] := parentY;}
\\[0.2cm]
otherwise we should use
\\[0.2cm]
\hspace*{1.3cm}
\texttt{parent[parentY] := parentX;}
\\[0.2cm]
In order to be able to distinguish these case, we store the height of the tree rooted at node
\texttt{n} in the relation \texttt{height}, i.e.~if \texttt{n} is a node, then \texttt{height[n]} is
the height of the tree rooted at node \texttt{n}.  This yields the implementation shown in Figure
\ref{fig:union-find.stlx} on page \pageref{fig:union-find.stlx}.  Provided the size  of the relation
\texttt{r} is bounded by the size $n$ of the set \texttt{m}, the complexity of this
implementation is $\Oh\bigl(n \cdot \log(n)\bigr)$.


\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                ]
    unionFind := procedure(m, r) {
        parent := { [n, n] : n in m };
        height := { [n, 0] : n in m };
        for ([x, y] in r) {
            parentX := find(x, parent);
            parentY := find(y, parent);
            if (parentX != parentY) {
                if (height[x] < height[y]) {
                    parent[parentX] := parentY;  
                } else if (height[x] > height[y]) {
                    parent[parentY] := parentX;  
                } else {
                    parent[parentY] := parentX;  
                    height[parentX] += 1;
                }
            }
        }
        roots := { x : x in m | parent[x] == x };
        return { { y : y in m | parent[y] == r } : r in roots };
    };
\end{Verbatim}
\vspace*{-0.3cm}
\caption{A more efficient version of the union-find algorithm.}
\label{fig:union-find.stlx}
\end{figure}

\exercise
We can speed up the implementation previously shown if the the set \texttt{m} has the form
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{m} = \{ 1, 2, 3, \cdots, n \}$ \quad where $n \in \mathbb{N}$.
\\[0.2cm]
In this case, the relations \texttt{parent} and \texttt{height} can be implemented as arrays.
Develop an implementation that is based on this idea.
\eox

\section[Shortest Paths]{Die Berechnung k\"urzester Wege}
Um das Problem der Berechnung k\"urzester Wege formulieren zu k\"onnen, f\"uhren wir zun\"achst 
den Begriff des \emph{gewichteten Graphen} ein.  

\begin{Definition}[Gewichteter Graph] \lb
  Ein  {\em gewichteter Graph} ist ein Tripel 
   $\langle \nodes, \edges, \weight{\cdot} \rangle$ so dass gilt:
  \begin{enumerate}
  \item $\nodes$ ist eine Menge von \emph{Knoten}.
  \item $\edges \subseteq \nodes \times \nodes$ ist eine Menge von \emph{Kanten}.
  \item $\weight{\cdot}: \edges \rightarrow \N \backslash\{0\}$ ist eine Funktion,
        die jeder Kante eine positive \emph{L\"ange} zuordnet.
        \conclude
  \end{enumerate}
\end{Definition}

\noindent
Ein \emph{Pfad} $P$ ist eine Liste der Form \\[0.2cm]
\hspace*{1.3cm} $P = [ x_1, x_2, x_3, \cdots, x_n ]$ \\[0.2cm]
so dass f\"ur alle $i = 1, \cdots, n-1$ gilt: \\[0.2cm]
\hspace*{1.3cm} $\pair(x_i,x_{i+1}) \in \edges$. \\[0.2cm]
Die Menge aller Pfade bezeichnen wir mit $\paths$.
Die L\"ange eines Pfads definieren wir als die Summe der L\"ange aller Kanten:
\\[0.2cm]
\hspace*{1.3cm} $\Weight{[x_1,x_2, \cdots, x_n]} \df \sum\limits_{i=1}^{n-1} \Weight{\pair(x_i,x_{i+1})}$. \\[0.2cm]
Ist $p = [x_1, x_2, \cdots, x_n]$ ein Pfad, so sagen wir, dass $p$ den Knoten $x_1$ mit dem
Knoten $x_n$ verbindet.   Die Menge alle Pfade, die den Knoten $v$ mit dem Knoten $w$
verbinden, bezeichnen wir als \\[0.2cm]
\hspace*{1.3cm} 
 $\paths(v,w) \df \bigl\{ [x_1, x_2, \cdots, x_n] \in \paths \mid x_1 = v \,\wedge\, x_n = w \}$.
\\[0.2cm]
Damit k\"onnen wir nun das Problem der Berechnung k\"urzester Wege formulieren.

\begin{Definition}[K\"urzeste-Wege-Problem] \lb
  Gegeben sei ein gewichteter Graph 
  $G = \langle \nodes, \edges, \weight{\cdot} \rangle$ 
  und ein  Knoten $\source \in \nodes$.  Dann besteht das 
  {\em k\"urzeste-Wege-Problem}  darin, die folgende Funktion zu berechnen: \\[0.2cm]
  \hspace*{1.3cm} $\spath: \nodes \rightarrow \N$ \\[0.1cm]
  \hspace*{1.3cm} $\spath(v) \df \mathtt{min}\bigl\{ \weight{p} \mid p \in \paths(\source,v) \bigr\}$.
  \conclude  
\end{Definition}

\subsection[Moore's Algorithm]{Der Algorithmus von Moore}
Wir betrachten zun\"achst den Algorithmus von Moore \cite{moore:59} zur Berechnung des k\"urzeste-Wege-Problems.
Abbildung \ref{fig:moore.stlx} zeigt eine Implementierung dieses Algorithmus' in \textsc{SetlX}.

\begin{figure}[!ht]
  \centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = -0.5cm,
                  xrightmargin  = -0.5cm
                ]
    shortestPath := procedure(source, edges) {
        dist   := { [source, 0] };
        fringe := { source };
        while (fringe != {}) {
            u := from(fringe);
            for ([v,l] in edges[u] | dist[v]==om || dist[u]+l<dist[v]) {
                dist[v] := dist[u] + l;
                fringe  += { v };
            }
        }
        return dist;
    };
\end{Verbatim}
\vspace*{-0.3cm}
  \caption{Algorithmus von Moore zur L\"osung des k\"urzeste-Wege-Problems.}
  \label{fig:moore.stlx}
\end{figure} 

\noindent
\begin{enumerate}
\item Die Funktion $\mathtt{shortestPath}(\textsl{source}, \textsl{edges})$ wird mit zwei Parametern
      aufgerufen:
      \begin{enumerate}
      \item \textsl{source} ist der Start-Knoten, von dem aus wir die Entfernungen zu den anderen Knoten
            berechnen.
      \item \textsl{edges} ist eine bin\"are Relation, die f\"ur jeden Knoten $x$ eine Menge von Paaren der
            Form
            \\[0.2cm]
            \hspace*{1.3cm}
            $\{ [y_1, l_1], \cdots, [y_n, l_n] \}$
            \\[0.2cm]
            speichert.  Die Idee dabei ist, dass f\"ur jeden in dieser Menge gespeicherten Knoten $y_i$
            eine Kante $\langle x, y_i \rangle$ der L\"ange $l_i$ existiert.
      \end{enumerate}
\item Die Variable \textsl{dist} speichert die Abstands-Funktions als bin\"are Relation, also
      als Menge von Paaren der Form $[x, \mathtt{sp}(x)]$.  Hierbei ist $x \in \nodes$ und
      $\mathtt{sp}(x)$ ist der Abstand, den der Knoten $x$ von dem Start-Knoten \textsl{source} hat.

      Der Knoten \textsl{source} hat von dem Knoten \textsl{source} offenbar den Abstand $0$
      und zu Beginn unserer Rechnung ist das auch alles, was wir wissen.  Daher wird die Relation
      \textsl{dist} in Zeile 2 mit dem Paar \texttt{[\textsl{source}, 0]} initialisiert.
\item Die Variable \textsl{fringe} enth\"alt alle die Knoten, von denen ausgehend wir als n\"achstes die
      Abst\"ande benachbarter Knoten berechnen sollten.  Am Anfang wissen wir nur von \textsl{source}
      den Abstand und daher ist dies der einzige Knoten, mit dem wir die Menge \textsl{fringe} 
      in Zeile 3 initialisieren.
\item Solange es nun Knoten gibt, von denen ausgehend wir neue Wege berechnen k\"onnen,
      w\"ahlen wir in Zeile 5 einen beliebigen Knoten aus der Menge \textsl{fringe} aus
      und entfernen ihn aus dieser Menge.  
\item Anschlie{\ss}end berechnen wir die Menge aller Knoten $v$, f\"ur die wir jetzt einen neuen
      Abstand gefunden haben:
      \begin{enumerate}
      \item Das sind einerseits die Knoten $v$, f\"ur welche die Funktion $\textsl{dist}(v)$ bisher noch
            undefiniert war, weil wir diese Knoten in unserer bisherigen Reechnung noch gar nicht gesehen
            haben.
      \item Andererseits sind dies aber auch Knoten, f\"ur die wir schon einen Abstand haben, der aber
            gr\"o{\ss}er ist als der Abstand des Weges, den wir erhalten, wenn wir die Knoten von $u$ aus
            besuchen. 
      \end{enumerate}
      F\"ur alle diese Knoten berechnen wir den Abstand und f\"ugen diese Knoten dann in die Menge 
      \textsl{fringe} ein.
\item Der Algorithmus terminiert, wenn die Menge \textsl{fringe} leer ist, denn dann haben wir alle
      Knoten abgeklappert.
\end{enumerate}

\subsection{Der Algorithmus von Dijkstra}
\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.0cm,
                  xrightmargin  = 0.0cm,
                ]
    shortestPath := procedure(source, edges) {
        dist    := { [source, 0] };
        fringe  := { [0, source] };
        visited := { source };
        while (fringe != {}) {
            [d, u]  := first(fringe);
            fringe  -= { [d, u] };
            for ([v,l] in edges[u] | dist[v]==om || d+l<dist[v]) {
                fringe  -= { [dist[v], v] };
                dist[v] := d + l;
                fringe  += { [d + l, v] };
            }
            visited += { u };
        }
        return dist;
    };
\end{Verbatim}
\vspace*{-0.3cm}
\caption{Der Algorithmus von Dijkstra zur L\"osung des k\"urzeste-Wege-Problems.}
\label{fig:dijkstra.stlx}
\end{figure}

\noindent
Im Algorithmus von Moore ist die Frage, in welcher Weise die Knoten aus der Menge
\textsl{fringe} ausgew\"ahlt werden, nicht weiter spezifiziert.  
Die Idee bei dem von Edsger W.~Dijkstra (1930 -- 2002) im Jahre 1959 ver\"offentlichten
Algorithmus \cite{dijkstra:59}
besteht darin, immer den Knoten auszuw\"ahlen, der den geringsten Abstand zu
dem Knoten \textsl{source} hat.
Dazu wird die Menge \textsl{fringe} nun als  eine Priorit\"ats-Warteschlange
implementiert.  Als Priorit\"aten w\"ahlen wir die Entfernungen zu dem Knoten \texttt{source}.
Abbildung \ref{fig:dijkstra.stlx} auf Seite \pageref{fig:dijkstra.stlx} zeigt die 
Implementierung des Algorithmus von Dijkstra zur Berechnung der k\"urzesten Wege in der Sprache
\textsc{SetlX}.

In dem Problem in Abbildung \ref{fig:dijkstra.stlx} taucht noch eine Variable mit dem Namen \textsl{visited} auf.
Diese Variable bezeichnet die Menge der Knoten, die der Algorithmus schon \textsl{besucht}
hat.  Genauer sind das die Knoten $u$, die aus der Priorit\"ats-Warteschlange \texttt{fringe}
entfernt wurden und f\"ur die dann anschlie{\ss}end in der \texttt{for}-Schleife, die in Zeile 8
beginnt, alle zu $u$ benachbarten Knoten untersucht wurden.  
Die Menge \textsl{visited} hat f\"ur
die eigentliche Implementierung des Algorithmus keine Bedeutung, denn die Variable \textsl{visited}
wird nur in Zeile 4 und Zeile 13 geschrieben, aber sie wird an keiner Stelle gelesen.
Ich habe die Variable \textsl{visited} nur deshalb eingef\"uhrt, damit ich eine Invariante formulieren
kann, die f\"ur den Beweis der Korrektheit des Algorithmus zentral ist.  Diese Invariante lautet
\\[0.2cm]
\hspace*{1.3cm}
$\forall u\in\mathtt{visited}: \textsl{dist}[u] = \textsl{sp}(u)$.
\\[0.2cm]
F\"ur alle Knoten aus \textsl{visited} liefert die Funktion \textsl{dist}() also bereits den
k\"urzesten Abstand zum Knoten \textsl{source}.  
\vspace*{0.1cm}

\noindent
\textbf{Beweis}: Wir zeigen durch Induktion, dass jedesmal wenn wir einen Knoten $u$ in die Menge
\textsl{visited} einf\"ugen, die Gleichung
\\[0.2cm]
\hspace*{1.3cm}
$\textsl{dist}[u] = \textsl{sp}(u)$ 
\\[0.2cm]
gilt.
In dem Programm gibt es genau zwei Stellen, an denen die Menge \textsl{visited} ver\"andert wird.
\begin{enumerate}
\item[I.A.:]  
      Zu Beginn enth\"alt die Menge $\textsl{visited}$ nur den Knoten \textsl{source} und offenbar gilt
      \\[0.2cm]
      \hspace*{1.3cm}
      $\textsl{sp}(\textsl{source}) = 0 = \textsl{dist}(\textsl{source})$.
      \\[0.2cm]
      Daher stimmt die Behauptung f\"ur alle Elemente, die zu Beginn Elemente der Menge \textsl{visited} sind.
\item[I.S.:]
      In Zeile 13 f\"ugen wir den Knoten $u$ in die Menge \textsl{visited} ein.
      Wir betrachten nun die Situation unmittelbar vor dem Einf\"ugen von $u$.
      Falls  $u$ bereits ein Elemente der Menge \textsl{visited} sein sollte, gilt die Behauptung
      nach Induktions-Voraussetzung.  
      Wir brauchen also nur den Fall betrachten, dass  $u$ vor dem Einf\"ugen noch kein Element der 
      Menge \textsl{visited} ist.

      Wir f\"uhren den weiteren Beweis nun indirekt und nehmen an, dass 
      \\[0.2cm]
      \hspace*{1.3cm} $\textsl{dist}(u) > \textsl{sp}(u)$
      \\[0.2cm]
      gilt.  Dann gibt es einen k\"urzesten Pfad 
      \\[0.2cm]
      \hspace*{1.3cm} $p = [ x_0 = \textsl{source}, x_1, \cdots, x_n = u ]$
      \\[0.2cm]
      von \textsl{source} nach $u$, der insgesamt die L\"ange $\textsl{sp}(u)$ hat.
      Es sei  $i\in\{0,\cdots,n-1\}$ der Index f\"ur den 
      \\[0.2cm]
      \hspace*{1.3cm}
      $x_0\in \textsl{visited}$, $\cdots$, $x_i\in \textsl{visited}$ \quad aber \quad $x_{i+1} \not\in \mathtt{Visited}$,
      \\[0.2cm]
      gilt, $x_i$ ist also der erste Knoten aus dem Pfad $p$, f\"ur den $x_{i+1}$ nicht mehr
      in der Menge
      \textsl{visited} liegt.  Nachdem $x_i$ in die Menge Visited eingef\"ugt wurde,
      wurde f\"ur alle Knoten, die mit $x_i$ \"uber eine Kante verbunden sind,
      die Funktion \textsl{dist} neu ausgerechnet.  Insbesondere
      wurde auch $\textsl{dist}[x_{i+1}]$ neu berechnet und der Knoten $x_{i+1}$ wurde 
      sp\"atestens zu diesem Zeitpunkt in die Menge \textsl{fringe} eingef\"ugt.
      Au{\ss}erdem wissen wir, dass $\textsl{dist}[x_{i+1}] = \textsl{sp}(x_{i+1})$ gilt,
      denn nach Induktions-Voraussetzung gilt $\textsl{dist}[x_i] = \textsl{sp}(x_i)$
      und die Kante $\pair(x_i,x_{i+1})$ ist Teil eines k\"urzesten Pfades von $x_i$ nach $x_{i+1}$.
      
      Da wir nun angenommen haben, dass $x_{i+1} \not\in \textsl{visited}$ ist,
      muss $x_{i+1}$ immer noch in der Pri\-ori\-t\"ats-Warteschlange \textsl{fringe} liegen.
      Also muss $\textsl{dist}[x_{i+1}] \geq \textsl{dist}[u]$ gelten,
      denn sonst w\"are $x_{i+1}$ vor $u$ aus der Priorit\"ats-Warteschlange entfernt worden.
      Wegen $\textsl{sp}(x_{i+1}) = \textsl{dist}[x_{i+1}]$ haben wir dann aber
      den Widerspruch 
      \\[0.2cm]
      \hspace*{1.3cm} 
      $\textsl{sp}(u) \geq \textsl{sp}(x_{i+1}) = \textsl{dist}[x_{i+1}] \geq \textsl{dist}[u] > \textsl{sp}(u)$.
      \\[0.2cm]
      Dieser Widerspruch zeigt, dass die Annahme $\textsl{dist}[u] > \textsl{sp}(u)$ falsch sein muss.  Da andererseits aber immer
      $\textsl{dist}[u] \geq \textsl{sp}(u)$ gilt, denn kein Pfad kann k\"urzer als der k\"urzeste Pfad sein, gilt insgesamt
      \\[0.2cm]
      \hspace*{1.3cm}
      $\textsl{dist}[u] = \textsl{sp}(u)$. \qed
\end{enumerate}

\subsection{Komplexit\"at}
Wenn ein Knoten $u$ aus der Warteschlange \textsl{fringe} entfernt wird, ist er anschlie{\ss}end ein Element der
Menge \textsl{visited} und aus der oben gezeigten Invariante folgt, dass dann 
\\[0.2cm]
\hspace*{1.3cm}
$\textsl{sp}(u) = \textsl{dist}[u]$
\\[0.2cm]
gilt.  Daraus folgt aber notwendigerweise, dass der Knoten $u$ nie wieder in die Priorit\"ats-Warteschlange
\textsl{fringe}
eingef\"ugt werden kann, denn ein Knoten $v$ wird nur dann in \textsl{fringe} neu eingef\"ugt, wenn
entweder die Funktion $\textsl{dist}[v]$ noch undefiniert ist, oder sich der Wert von
$\texttt{dist}[v]$ verkleinert.  
Das Einf\"ugen eines Knoten in eine Priorit\"ats-Warteschlange mit $n$
Elementen kostet eine Rechenzeit, die durch $\Oh\bigl(\log_2(n)\bigr)$ abgesch\"atzt werden kann.  Da die
Warteschlange sicher nie mehr als $\#\nodes$ Knoten enthalten kann und da jeder Knoten h\"ochstens einmal eingef\"ugt
werden kann, liefert das einen Term der Form 
\\[0.2cm]
\hspace*{1.3cm}
$\Oh\bigl(\#V \cdot \log_2(\#V)\bigr)$ 
\\[0.2cm]
f\"ur das Einf\"ugen der Knoten.  Neben dem Einf\"ugen eines Knotens durch den Befehl
\\[0.2cm]
\hspace*{1.3cm}
\texttt{fringe += \{ [dvNew, v] \};}
\\[0.2cm]
m\"ussen wir auch die Komplexit\"at des Aufrufs
\\[0.2cm]
\hspace*{1.3cm}
\texttt{fringe -= \{ [dvOld, v] \};} 
\\[0.2cm]
analysieren.
Die Anzahl dieser Aufrufe ist durch die Anzahl der Kanten begrenzt, die zu dem Knoten $v$ hinf\"uhren.
Da das Entfernen eines Elements aus einer Menge mit $n$ Elementen eine Rechenzeit
der Gr\"o{\ss}e $\Oh\bigl(\log_2(n)\bigr)$ erfordert, haben wir die Absch\"atzung
\\[0.2cm]
\hspace*{1.3cm}
$\Oh\bigl(\#\edges \cdot \log_2(\#\nodes)\bigr)$
\\[0.2cm]
f\"ur diese Rechenzeit.
Dabei bezeichnet $\#\edges$ die Anzahl der Kanten. Damit erhalten wir f\"ur die
Komplexit\"at von Dijkstra's Algorithmus insgesamt den Ausdruck \\[0.2cm]
\hspace*{1.3cm} $\Oh\bigl((\#\edges + \#\nodes) * \ln(\#\nodes)\bigr)$. \\[0.2cm]
Ist die Zahl der Kanten, die von den Knoten ausgehen k\"onnen, durch eine feste Zahl begrenzt
(z.B. wenn von jedem Knoten nur maximal 4 Kanten ausgehen), so
kann  die Gesamt-Zahl der Kanten durch ein festes Vielfaches der Knoten-Zahl abgesch\"atzt
werden.  Dann ist  die Komplexit\"at f\"ur Dijkstra's Algorithmus zur  Bestimmung der k\"urzesten Wege
durch den Ausdruck  
\\[0.2cm]
\hspace*{1.3cm}
$\Oh\bigl(\#\nodes * \log_2(\#\nodes)\bigr)$ 
\\[0.2cm]
gegeben.




%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "algorithms"
%%% End: 
